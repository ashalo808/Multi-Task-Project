{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e2a0ef9",
   "metadata": {},
   "source": [
    "# 1 下载bert-base-chinese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07c15e64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/multi/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "本地模型已存在于 ./bert-base-chinese-local，直接加载\n",
      "模型已从本地成功加载\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"HF_ENDPOINT\"] = \"https://hf-mirror.com\"\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# 设置模型保存目录\n",
    "save_directory = \"./bert-base-chinese-local\"\n",
    "\n",
    "# 检查模型是否已存在\n",
    "if os.path.exists(os.path.join(save_directory, \"tokenizer_config.json\")) and \\\n",
    "   os.path.exists(os.path.join(save_directory, \"config.json\")):\n",
    "    print(f\"本地模型已存在于 {save_directory}，直接加载\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(save_directory, local_files_only=True)\n",
    "    model = AutoModel.from_pretrained(save_directory, local_files_only=True)\n",
    "    print(\"模型已从本地成功加载\")\n",
    "else:\n",
    "    print(f\"警告：本地模型目录 {save_directory} 不完整，请确保已正确下载\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93290e40",
   "metadata": {},
   "source": [
    "# 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6eae2bae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added /root/Code/Multi-task Project to system path\n",
      "Current working directory: /root/Code/Multi-task Project\n",
      "Python path: ['/root/miniconda3/envs/multi/lib/python310.zip', '/root/miniconda3/envs/multi/lib/python3.10', '/root/miniconda3/envs/multi/lib/python3.10/lib-dynload', '', '/root/miniconda3/envs/multi/lib/python3.10/site-packages', '/tmp/tmpm5exwty3', '/root/Code/Multi-task Project']\n",
      "src directory found: /root/Code/Multi-task Project/src\n",
      "Models directory found: /root/Code/Multi-task Project/src/models\n",
      "Files in models directory:\n",
      "  - core_seq2seq.py\n",
      "  - __init__.py\n",
      "  - __pycache__\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# 修改: 添加项目根目录到Python路径 (适应本地环境)\n",
    "project_root = os.path.dirname(os.path.abspath('__file__'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "    print(f\"Added {project_root} to system path\")\n",
    "\n",
    "# 打印当前工作目录和Python路径，用于调试\n",
    "print(f\"Current working directory: {os.getcwd()}\")\n",
    "print(f\"Python path: {sys.path}\")\n",
    "\n",
    "# 检查src目录是否存在\n",
    "src_path = os.path.join(project_root, 'src')\n",
    "if os.path.exists(src_path):\n",
    "    print(f\"src directory found: {src_path}\")\n",
    "else:\n",
    "    print(f\"Warning: src directory not found at {src_path}\")\n",
    "\n",
    "# 显示src/models目录的内容\n",
    "models_path = os.path.join(src_path, 'models')\n",
    "if os.path.exists(models_path):\n",
    "    print(f\"Models directory found: {models_path}\")\n",
    "    print(\"Files in models directory:\")\n",
    "    for f in os.listdir(models_path):\n",
    "        print(f\"  - {f}\")\n",
    "else:\n",
    "    print(f\"Warning: models directory not found at {models_path}\")\n",
    "\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47371eec",
   "metadata": {},
   "source": [
    "# 3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b417bbe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup\n",
    "from torch.optim import AdamW\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from tqdm import tqdm\n",
    "from IPython.display import display, HTML\n",
    "import sys\n",
    "\n",
    "# 添加项目根目录到Python路径\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "sys.path.append(project_root)\n",
    "\n",
    "# 导入我们自己的Seq2SeqTransformer模型\n",
    "from src.models.core_seq2seq import Seq2SeqTransformer, PositionalEncoding\n",
    "\n",
    "# 设置随机种子以保证可重复性\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "# 检测设备\n",
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        print(f\"使用GPU: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"GPU内存总量: {torch.cuda.get_device_properties(0).total_memory / 1024 / 1024 / 1024:.2f} GB\")\n",
    "        print(f\"可用GPU数量: {torch.cuda.device_count()}\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"使用CPU\")\n",
    "    \n",
    "    return device\n",
    "\n",
    "# 流式加载JSON数据并随机抽样\n",
    "def load_json_data_with_sampling(file_path, sample_percentage=10, max_samples=None, task_types=None):\n",
    "    sample_prob = sample_percentage / 100\n",
    "    sampled_data = []\n",
    "    line_count = 0\n",
    "    \n",
    "    print(f\"Reading file and sampling {sample_percentage}% of data...\")\n",
    "    \n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in tqdm(f):\n",
    "            line_count += 1\n",
    "            \n",
    "            if random.random() <= sample_prob:\n",
    "                try:\n",
    "                    item = json.loads(line.strip())\n",
    "                    \n",
    "                    if task_types is not None and item.get('type') not in task_types:\n",
    "                        continue\n",
    "                        \n",
    "                    sampled_data.append(item)\n",
    "                    \n",
    "                    if max_samples and len(sampled_data) >= max_samples:\n",
    "                        break\n",
    "                except json.JSONDecodeError:\n",
    "                    continue\n",
    "\n",
    "    print(f\"Total lines read: {line_count}\")\n",
    "    print(f\"Sampled data size: {len(sampled_data)}\")\n",
    "    \n",
    "    return sampled_data\n",
    "\n",
    "# 检查标签配置\n",
    "def fix_label_mapping(data, task_types):\n",
    "    task_specific_labels = {t: set() for t in task_types}\n",
    "    \n",
    "    # 统计每种任务的标签\n",
    "    for item in data:\n",
    "        task = item['type']\n",
    "        if task in task_types and 'answer_choices' in item:\n",
    "            for choice in item['answer_choices']:\n",
    "                task_specific_labels[task].add(choice)\n",
    "            \n",
    "    print(\"任务特定标签统计:\")\n",
    "    for task, labels in task_specific_labels.items():\n",
    "        print(f\"{task}: {len(labels)} 个标签 - {list(labels)[:5]}...\")\n",
    "\n",
    "# 自定义数据集类 - 修改以适应多任务模型\n",
    "class MultiTaskDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length=512, pad_idx=0):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.pad_idx = pad_idx\n",
    "        \n",
    "        # 按任务类型分类数据\n",
    "        self.task_data = {\n",
    "            'classify': [],\n",
    "            'nli': [],\n",
    "            'mrc': []\n",
    "        }\n",
    "        \n",
    "        for item in data:\n",
    "            task_type = item['type']\n",
    "            if task_type in self.task_data:\n",
    "                self.task_data[task_type].append(item)\n",
    "            \n",
    "        # 统计各任务数据量\n",
    "        self.task_counts = {task: len(items) for task, items in self.task_data.items()}\n",
    "        print(f\"Task data distribution: {self.task_counts}\")\n",
    "        \n",
    "        # 为分类任务和NLI任务创建标签映射\n",
    "        self.label_maps = {}\n",
    "        self._create_label_maps()\n",
    "        \n",
    "    def _create_label_maps(self):\n",
    "        \"\"\"为每个任务创建标签映射\"\"\"\n",
    "        for task_type in ['classify', 'nli']:\n",
    "            all_labels = set()\n",
    "            for item in self.task_data[task_type]:\n",
    "                if 'answer_choices' in item and item['answer_choices']:\n",
    "                    for choice in item['answer_choices']:\n",
    "                        all_labels.add(choice)\n",
    "                        \n",
    "            # 为NLI任务标准化标签\n",
    "            if task_type == 'nli':\n",
    "                nli_map = {\n",
    "                    'entailment': 'entailment', '蕴含': 'entailment', '是的': 'entailment', 'yes': 'entailment',\n",
    "                    'neutral': 'neutral', '中立': 'neutral',\n",
    "                    'contradiction': 'contradiction', '矛盾': 'contradiction', '不是': 'contradiction', 'no': 'contradiction'\n",
    "                }\n",
    "                # 应用映射\n",
    "                all_labels = set(nli_map.get(label, label) for label in all_labels)\n",
    "                \n",
    "            self.label_maps[task_type] = {label: i for i, label in enumerate(sorted(all_labels))}\n",
    "            print(f\"{task_type} task has {len(self.label_maps[task_type])} unique labels\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return sum(self.task_counts.values())\n",
    "    \n",
    "    def get_task_item(self, task_type, idx):\n",
    "        \"\"\"获取特定任务类型的样本\"\"\"\n",
    "        if idx >= len(self.task_data[task_type]):\n",
    "            return None\n",
    "            \n",
    "        item = self.task_data[task_type][idx]\n",
    "        \n",
    "        # 根据任务类型进行不同处理\n",
    "        if task_type in ['classify', 'nli']:\n",
    "            return self._process_classification_item(item, task_type)\n",
    "        elif task_type == 'mrc':\n",
    "            return self._process_mrc_item(item)\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    def _process_classification_item(self, item, task_type):\n",
    "        \"\"\"处理分类和NLI任务的数据项\"\"\"\n",
    "        input_text = item['input']\n",
    "        target_text = item['target']\n",
    "        answer_choices = item.get('answer_choices', [])\n",
    "        \n",
    "        # 对NLI任务的特殊处理\n",
    "        if task_type == 'nli':\n",
    "            nli_map = {\n",
    "                'entailment': 0, '蕴含': 0, '是的': 0, 'yes': 0,\n",
    "                'neutral': 1, '中立': 1,\n",
    "                'contradiction': 2, '矛盾': 2, '不是': 2, 'no': 2\n",
    "            }\n",
    "            \n",
    "            if target_text in nli_map:\n",
    "                label = torch.tensor(nli_map[target_text], dtype=torch.long)\n",
    "            elif target_text in answer_choices:\n",
    "                # 二分类NLI问题(是/否)\n",
    "                if len(answer_choices) == 2:\n",
    "                    label = torch.tensor(0 if target_text == answer_choices[0] else 1, dtype=torch.long)\n",
    "                else:\n",
    "                    label = torch.tensor(answer_choices.index(target_text), dtype=torch.long)\n",
    "            else:\n",
    "                label = torch.tensor(0, dtype=torch.long)  # 默认值\n",
    "        else:\n",
    "            # 分类任务处理\n",
    "            try:\n",
    "                if target_text in self.label_maps[task_type]:\n",
    "                    label_idx = self.label_maps[task_type][target_text]\n",
    "                elif target_text in answer_choices:\n",
    "                    label_idx = answer_choices.index(target_text)\n",
    "                else:\n",
    "                    label_idx = 0\n",
    "                label = torch.tensor(label_idx, dtype=torch.long)\n",
    "            except (ValueError, KeyError):\n",
    "                label = torch.tensor(0, dtype=torch.long)\n",
    "        \n",
    "        # 数据增强 (可选)\n",
    "        if task_type == 'classify' and random.random() < 0.2:\n",
    "            input_text = self._augment_text(input_text)\n",
    "            \n",
    "        # Tokenization\n",
    "        encoding = self.tokenizer(\n",
    "            input_text,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt',\n",
    "            return_token_type_ids=True\n",
    "        )\n",
    "        \n",
    "        # 去掉批次维度\n",
    "        for key in encoding:\n",
    "            encoding[key] = encoding[key].squeeze(0)\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'],\n",
    "            'attention_mask': encoding['attention_mask'],\n",
    "            'token_type_ids': encoding.get('token_type_ids', None),\n",
    "            'src_padding_mask': (encoding['input_ids'] == self.pad_idx),\n",
    "            'label': label,\n",
    "            'task_type': task_type,\n",
    "            'target_text': target_text,\n",
    "            'answer_choices': answer_choices\n",
    "        }\n",
    "        \n",
    "    def _process_mrc_item(self, item):\n",
    "        \"\"\"处理MRC任务的数据项 - 改进版\"\"\"\n",
    "        input_text = item['input']\n",
    "        target_text = item['target'].strip()\n",
    "        \n",
    "        # 处理上下文和问题\n",
    "        context = input_text.split('[SEP]')[0].strip() if '[SEP]' in input_text else input_text\n",
    "        \n",
    "        # 使用Transformers的精确偏移映射\n",
    "        encoding = self.tokenizer(\n",
    "            input_text,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt',\n",
    "            return_offsets_mapping=True,\n",
    "            return_token_type_ids=True\n",
    "        )\n",
    "        \n",
    "        # 提取偏移映射\n",
    "        offset_mapping = encoding.pop('offset_mapping')[0].numpy()\n",
    "        \n",
    "        # 找到答案在上下文中的位置\n",
    "        start_idx = context.find(target_text)\n",
    "        start_position = None\n",
    "        end_position = None\n",
    "        \n",
    "        if start_idx != -1:\n",
    "            end_idx = start_idx + len(target_text)\n",
    "            \n",
    "            # 找到对应的token位置\n",
    "            start_token = None\n",
    "            end_token = None\n",
    "            \n",
    "            for idx, (token_start, token_end) in enumerate(offset_mapping):\n",
    "                # 找起始token\n",
    "                if token_start <= start_idx < token_end:\n",
    "                    start_token = idx\n",
    "                # 找结束token\n",
    "                if token_start < end_idx <= token_end and start_token is not None:\n",
    "                    end_token = idx\n",
    "                    break\n",
    "            \n",
    "            if start_token is not None and end_token is not None:\n",
    "                start_position = torch.tensor(start_token, dtype=torch.long)\n",
    "                end_position = torch.tensor(end_token, dtype=torch.long)\n",
    "            else:\n",
    "                # 备用方法：寻找包含答案的最小token范围\n",
    "                answer_tokens = self.tokenizer.encode(\n",
    "                    target_text, add_special_tokens=False, return_tensors='pt'\n",
    "                )[0]\n",
    "                \n",
    "                input_ids = encoding['input_ids'][0].tolist()\n",
    "                for i in range(len(input_ids) - len(answer_tokens) + 1):\n",
    "                    if input_ids[i:i+len(answer_tokens)] == answer_tokens.tolist():\n",
    "                        start_position = torch.tensor(i, dtype=torch.long)\n",
    "                        end_position = torch.tensor(i + len(answer_tokens) - 1, dtype=torch.long)\n",
    "                        break\n",
    "        \n",
    "        # 如果仍然找不到，使用默认位置\n",
    "        if start_position is None or end_position is None:\n",
    "            # 更合理的默认位置：指向[CLS]标记后的第一个实际token\n",
    "            start_position = torch.tensor(1, dtype=torch.long)\n",
    "            end_position = torch.tensor(1, dtype=torch.long)\n",
    "        \n",
    "        # 去掉批次维度\n",
    "        for key in encoding:\n",
    "            if isinstance(encoding[key], torch.Tensor):\n",
    "                encoding[key] = encoding[key].squeeze(0)\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'],\n",
    "            'attention_mask': encoding['attention_mask'],\n",
    "            'token_type_ids': encoding.get('token_type_ids', None),\n",
    "            'src_padding_mask': (encoding['input_ids'] == self.pad_idx),\n",
    "            'start_position': start_position,\n",
    "            'end_position': end_position,\n",
    "            'task_type': 'mrc',\n",
    "            'target_text': target_text\n",
    "        }\n",
    "        \n",
    "    def _augment_text(self, text):\n",
    "        \"\"\"文本增强函数\"\"\"\n",
    "        words = list(text)\n",
    "        # 随机替换、删除或插入字符\n",
    "        for i in range(min(3, len(words))):\n",
    "            pos = random.randint(0, len(words) - 1)\n",
    "            if random.random() < 0.5:  # 替换\n",
    "                words[pos] = random.choice(\"的地得了吗呢啊哦嗯呀哈\")\n",
    "            else:  # 删除\n",
    "                if len(words) > 10:  # 确保不会删除太多\n",
    "                    words.pop(pos)\n",
    "        return ''.join(words)\n",
    "    \n",
    "    def _augment_mrc(self, input_text, target_text, context):\n",
    "        \"\"\"专门为MRC任务设计的数据增强\"\"\"\n",
    "        # 查找上下文中答案的位置\n",
    "        start_idx = context.find(target_text)\n",
    "        if start_idx == -1:\n",
    "            return input_text, target_text\n",
    "        \n",
    "        # 如果答案在文本中，有50%的概率保持不变\n",
    "        if random.random() < 0.5:\n",
    "            return input_text, target_text\n",
    "        \n",
    "        # 对答案进行轻微变换\n",
    "        aug_operations = [\n",
    "            # 添加前后缀\n",
    "            lambda t: t + random.choice([\"。\", \"，\", \"的\"]) if len(t) > 2 else t,\n",
    "            # 删除末尾字符\n",
    "            lambda t: t[:-1] if len(t) > 2 else t,\n",
    "            # 替换一个字符\n",
    "            lambda t: t if len(t) <= 3 else t[:random.randint(1,len(t)-2)] + random.choice(\"的一是在\") + t[random.randint(1,len(t)-2)+1:]\n",
    "        ]\n",
    "        \n",
    "        # 随机选择一种增强\n",
    "        aug_op = random.choice(aug_operations)\n",
    "        new_target = aug_op(target_text)\n",
    "        \n",
    "        # 替换原始上下文中的答案\n",
    "        if new_target != target_text and len(new_target) > 0:\n",
    "            new_context = context[:start_idx] + new_target + context[start_idx+len(target_text):]\n",
    "            new_input = new_context\n",
    "            if '[SEP]' in input_text:\n",
    "                question = input_text.split('[SEP]')[1]\n",
    "                new_input = new_context + ' [SEP] ' + question\n",
    "                \n",
    "            return new_input, new_target\n",
    "            \n",
    "        return input_text, target_text\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"根据索引获取样本，但按任务类型分配索引\"\"\"\n",
    "        # 计算每个任务的样本量比例\n",
    "        total = self.__len__()\n",
    "        task_probs = {task: count/total for task, count in self.task_counts.items()}\n",
    "        \n",
    "        # 根据比例随机选择任务类型\n",
    "        task_type = random.choices(\n",
    "            list(task_probs.keys()),\n",
    "            weights=list(task_probs.values()),\n",
    "            k=1\n",
    "        )[0]\n",
    "        \n",
    "        # 为选中的任务类型随机选择一个索引\n",
    "        task_idx = random.randint(0, self.task_counts[task_type] - 1)\n",
    "        \n",
    "        try:\n",
    "            return self.get_task_item(task_type, task_idx)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {task_type} item at index {task_idx}: {e}\")\n",
    "            # 返回一个默认项避免批处理失败\n",
    "            return self.get_task_item('classify', 0) or None\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    \"\"\"处理不同大小的批次数据\"\"\"\n",
    "    # 过滤掉None值\n",
    "    batch = [item for item in batch if item is not None]\n",
    "    \n",
    "    if len(batch) == 0:\n",
    "        return {}\n",
    "    \n",
    "    # 首先收集所有样本中出现的键\n",
    "    all_keys = set()\n",
    "    for item in batch:\n",
    "        all_keys.update(item.keys())\n",
    "    \n",
    "    result = {}\n",
    "    for key in all_keys:\n",
    "        # 只处理在所有样本中都存在的键，或特殊处理某些键\n",
    "        if all(key in item for item in batch):\n",
    "            # 对于字符串或列表类型的字段\n",
    "            if key in ['task_type', 'target_text', 'answer_choices']:\n",
    "                result[key] = [item[key] for item in batch]\n",
    "            # 对于张量类型\n",
    "            elif isinstance(batch[0][key], torch.Tensor):\n",
    "                shapes = [item[key].shape for item in batch]\n",
    "                if all(shape == shapes[0] for shape in shapes):\n",
    "                    # 如果所有形状一致，则进行常规的堆叠\n",
    "                    result[key] = torch.stack([item[key] for item in batch])\n",
    "                else:\n",
    "                    # 如果形状不一致，则进行填充\n",
    "                    max_len = max(shape[0] for shape in shapes)\n",
    "                    padded_tensors = []\n",
    "                    for item in batch:\n",
    "                        tensor = item[key]\n",
    "                        if tensor.shape[0] < max_len:\n",
    "                            padding = torch.zeros(max_len - tensor.shape[0], *tensor.shape[1:], \n",
    "                                                dtype=tensor.dtype, device=tensor.device)\n",
    "                            tensor = torch.cat([tensor, padding], dim=0)\n",
    "                        padded_tensors.append(tensor)\n",
    "                    result[key] = torch.stack(padded_tensors)\n",
    "        else:\n",
    "            # 特殊处理MRC任务特有的字段\n",
    "            if key in ['start_position', 'end_position']:\n",
    "                # 找出拥有该键的样本索引\n",
    "                indices = [i for i, item in enumerate(batch) if key in item]\n",
    "                if indices:\n",
    "                    # 创建包含有效值的列表\n",
    "                    values = [batch[i][key] for i in indices]\n",
    "                    # 记录这些样本的索引，以便训练时使用\n",
    "                    result[f'{key}_indices'] = indices\n",
    "                    result[key] = torch.stack(values)\n",
    "    \n",
    "    return result\n",
    "\n",
    "# 数据加载器创建函数\n",
    "def create_dataloaders(train_data, val_data, tokenizer, batch_size=8):\n",
    "    train_dataset = MultiTaskDataset(train_data, tokenizer)\n",
    "    val_dataset = MultiTaskDataset(val_data, tokenizer)\n",
    "    \n",
    "    # 使用num_workers加速数据加载\n",
    "    num_workers = min(4, os.cpu_count())\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=True, \n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,  # GPU加速\n",
    "        collate_fn=custom_collate_fn  # 使用自定义的collate函数\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, \n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,  # GPU加速\n",
    "        collate_fn=custom_collate_fn  # 使用自定义的collate函数\n",
    "    )\n",
    "    \n",
    "    return train_loader, val_loader, train_dataset.label_map\n",
    "\n",
    "def create_task_specific_dataloaders(train_data, val_data, tokenizer, batch_size=8):\n",
    "    \"\"\"创建按任务类型分类的数据加载器\"\"\"\n",
    "    train_dataset = MultiTaskDataset(train_data, tokenizer)\n",
    "    val_dataset = MultiTaskDataset(val_data, tokenizer)\n",
    "    \n",
    "    # 使用num_workers加速数据加载\n",
    "    num_workers = min(4, os.cpu_count())\n",
    "    \n",
    "    # 创建任务特定的数据加载器\n",
    "    train_loaders = {}\n",
    "    val_loaders = {}\n",
    "    \n",
    "    for task_type in ['classify', 'nli', 'mrc']:\n",
    "        if train_dataset.task_counts[task_type] > 0:\n",
    "            # 创建任务特定的数据集包装器\n",
    "            train_task_dataset = TaskSpecificDataset(train_dataset, task_type)\n",
    "            train_loaders[task_type] = DataLoader(\n",
    "                train_task_dataset,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=True,\n",
    "                num_workers=num_workers,\n",
    "                pin_memory=True,\n",
    "                collate_fn=lambda x: task_collate_fn(x, task_type)\n",
    "            )\n",
    "        \n",
    "        if val_dataset.task_counts[task_type] > 0:\n",
    "            val_task_dataset = TaskSpecificDataset(val_dataset, task_type)\n",
    "            val_loaders[task_type] = DataLoader(\n",
    "                val_task_dataset,\n",
    "                batch_size=batch_size,\n",
    "                num_workers=num_workers,\n",
    "                pin_memory=True,\n",
    "                collate_fn=lambda x: task_collate_fn(x, task_type)\n",
    "            )\n",
    "    \n",
    "    return train_loaders, val_loaders, train_dataset.label_maps\n",
    "\n",
    "class TaskSpecificDataset(Dataset):\n",
    "    \"\"\"任务特定的数据集包装器\"\"\"\n",
    "    def __init__(self, multi_task_dataset, task_type):\n",
    "        self.dataset = multi_task_dataset\n",
    "        self.task_type = task_type\n",
    "        self.indices = list(range(self.dataset.task_counts[task_type]))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.dataset.get_task_item(self.task_type, self.indices[idx])\n",
    "\n",
    "# 创建任务特定的数据加载器\n",
    "def task_collate_fn(batch, task_type):\n",
    "    \"\"\"任务特定的批处理函数\"\"\"\n",
    "    # 过滤掉None值\n",
    "    batch = [item for item in batch if item is not None]\n",
    "    \n",
    "    if len(batch) == 0:\n",
    "        return {}\n",
    "    \n",
    "    result = {}\n",
    "    \n",
    "    # 常规键处理\n",
    "    for key in batch[0].keys():\n",
    "        if key in ['task_type', 'target_text', 'answer_choices']:\n",
    "            result[key] = [item[key] for item in batch]\n",
    "        elif isinstance(batch[0][key], torch.Tensor):\n",
    "            result[key] = torch.stack([item[key] for item in batch])\n",
    "    \n",
    "    # MRC任务特殊处理\n",
    "    if task_type == 'mrc':\n",
    "        if all('start_position' in item and 'end_position' in item for item in batch):\n",
    "            result['start_position'] = torch.stack([item['start_position'] for item in batch])\n",
    "            result['end_position'] = torch.stack([item['end_position'] for item in batch])\n",
    "    \n",
    "    return result\n",
    "\n",
    "class LabelSmoothingCrossEntropy(nn.Module):\n",
    "    def __init__(self, smoothing=0.1):\n",
    "        super(LabelSmoothingCrossEntropy, self).__init__()\n",
    "        self.smoothing = smoothing\n",
    "        \n",
    "    def forward(self, x, target):\n",
    "        confidence = 1. - self.smoothing\n",
    "        logprobs = F.log_softmax(x, dim=-1)\n",
    "        nll_loss = -logprobs.gather(dim=-1, index=target.unsqueeze(1))\n",
    "        nll_loss = nll_loss.squeeze(1)\n",
    "        smooth_loss = -logprobs.mean(dim=-1)\n",
    "        loss = confidence * nll_loss + self.smoothing * smooth_loss\n",
    "        return loss.mean()\n",
    "\n",
    "# 使用预训练的BERT替代自定义编码器\n",
    "class MultitaskBertModel(nn.Module):\n",
    "    def __init__(self, num_labels=2):\n",
    "        super(MultitaskBertModel, self).__init__()\n",
    "        \n",
    "        # 加载预训练的BERT模型\n",
    "        self.bert = AutoModel.from_pretrained(\"./bert-base-chinese-local\")\n",
    "        \n",
    "        # 任务特定的头部\n",
    "        self.classify_head = nn.Linear(self.bert.config.hidden_size, num_labels)\n",
    "        self.nli_head = nn.Linear(self.bert.config.hidden_size, num_labels)\n",
    "        \n",
    "        # 改进MRC头部设计\n",
    "        hidden_size = self.bert.config.hidden_size\n",
    "        self.mrc_qa_outputs = nn.Linear(hidden_size, 2)\n",
    "        \n",
    "        # 添加MRC专用的双向LSTM层增强上下文理解\n",
    "        self.mrc_lstm = nn.LSTM(\n",
    "            input_size=hidden_size,\n",
    "            hidden_size=hidden_size//2,  # 双向所以除2\n",
    "            num_layers=1,\n",
    "            bidirectional=True,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # 初始化权重\n",
    "        nn.init.xavier_uniform_(self.classify_head.weight)\n",
    "        nn.init.zeros_(self.classify_head.bias)\n",
    "\n",
    "        nn.init.xavier_uniform_(self.nli_head.weight)\n",
    "        nn.init.zeros_(self.nli_head.bias)\n",
    "\n",
    "        nn.init.xavier_uniform_(self.mrc_qa_outputs.weight)\n",
    "        nn.init.zeros_(self.mrc_qa_outputs.bias)\n",
    "        \n",
    "        # 冻结BERT底层参数但保持顶层可训练\n",
    "        for param in list(self.bert.parameters())[:-4]:\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    def _process_mrc(self, sequence_output, attention_mask):\n",
    "        \"\"\"处理MRC任务的序列输出\"\"\"\n",
    "        # 首先通过LSTM增强上下文理解\n",
    "        lstm_output, _ = self.mrc_lstm(sequence_output)\n",
    "        \n",
    "        # 然后使用QA输出层获取起始和结束位置\n",
    "        logits = self.mrc_qa_outputs(lstm_output)\n",
    "        start_logits, end_logits = logits.split(1, dim=-1)\n",
    "        \n",
    "        # 去除多余维度\n",
    "        start_logits = start_logits.squeeze(-1)\n",
    "        end_logits = end_logits.squeeze(-1)\n",
    "        \n",
    "        # 应用注意力掩码\n",
    "        if attention_mask is not None:\n",
    "            # 确保不在有效范围内的位置不被选为答案\n",
    "            start_logits = start_logits + (1.0 - attention_mask) * -10000.0\n",
    "            end_logits = end_logits + (1.0 - attention_mask) * -10000.0\n",
    "        \n",
    "        return start_logits, end_logits\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask=None, token_type_ids=None, task_type=None):\n",
    "        # 使用BERT提取特征\n",
    "        outputs = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids\n",
    "        )\n",
    "        \n",
    "        # 获取序列表示和[CLS]的表示\n",
    "        sequence_output = outputs.last_hidden_state  # 用于MRC任务\n",
    "        pooled_output = outputs.pooler_output  # 用于分类任务\n",
    "        \n",
    "        if isinstance(task_type, list):\n",
    "            # 批处理多个样本时使用\n",
    "            results = []\n",
    "            mrc_indices = []\n",
    "            \n",
    "            for i, t in enumerate(task_type):\n",
    "                if t == 'classify':\n",
    "                    results.append(self.classify_head(pooled_output[i:i+1]))\n",
    "                elif t == 'nli':\n",
    "                    results.append(self.nli_head(pooled_output[i:i+1]))\n",
    "                elif t == 'mrc':\n",
    "                    # 记录MRC样本的索引，稍后单独处理\n",
    "                    mrc_indices.append(i)\n",
    "                    # 暂时放入一个占位符\n",
    "                    dummy = torch.zeros(1, self.classify_head.out_features, device=pooled_output.device)\n",
    "                    results.append(dummy)\n",
    "                else:\n",
    "                    results.append(self.classify_head(pooled_output[i:i+1]))\n",
    "                    \n",
    "            # 如果没有MRC样本，直接返回结果\n",
    "            if not mrc_indices:\n",
    "                return torch.cat(results, dim=0)\n",
    "            \n",
    "            # 处理MRC样本\n",
    "            mrc_outputs = []\n",
    "            for i in mrc_indices:\n",
    "                mrc_input = input_ids[i:i+1]\n",
    "                mrc_mask = attention_mask[i:i+1] if attention_mask is not None else None\n",
    "                mrc_token_type = token_type_ids[i:i+1] if token_type_ids is not None else None\n",
    "                \n",
    "                start_logits, end_logits = self._process_mrc(\n",
    "                    sequence_output[i:i+1], \n",
    "                    mrc_mask\n",
    "                )\n",
    "                \n",
    "                mrc_outputs.append((start_logits, end_logits))\n",
    "                \n",
    "            # 最终返回包括MRC结果的元组\n",
    "            return torch.cat(results, dim=0), mrc_outputs\n",
    "        else:\n",
    "            # 单个样本或同类型批次处理\n",
    "            if task_type == 'classify':\n",
    "                return self.classify_head(pooled_output)\n",
    "            elif task_type == 'nli':\n",
    "                return self.nli_head(pooled_output)\n",
    "            elif task_type == 'mrc':\n",
    "                # 使用改进的MRC处理\n",
    "                # 首先通过LSTM增强上下文理解\n",
    "                lstm_output, _ = self.mrc_lstm(sequence_output)\n",
    "                \n",
    "                # 然后使用QA输出层获取起始和结束位置\n",
    "                logits = self.mrc_qa_outputs(lstm_output)\n",
    "                start_logits, end_logits = logits.split(1, dim=-1)\n",
    "                \n",
    "                # 去除多余维度\n",
    "                start_logits = start_logits.squeeze(-1)\n",
    "                end_logits = end_logits.squeeze(-1)\n",
    "                \n",
    "                # 应用掩码确保只在有效范围内预测\n",
    "                if attention_mask is not None:\n",
    "                    start_logits = start_logits + (1 - attention_mask) * -10000.0\n",
    "                    end_logits = end_logits + (1 - attention_mask) * -10000.0\n",
    "                \n",
    "                return start_logits, end_logits\n",
    "            else:\n",
    "                return self.classify_head(pooled_output)\n",
    "\n",
    "# 训练函数 - 优化以适用于GPU\n",
    "def train(model, train_loader, val_loader, optimizer, scheduler, device, best_model_path, tokenizer, num_epochs=3, eval_steps=100):\n",
    "    best_val_score = 0\n",
    "    global_step = 0\n",
    "    scaler = torch.cuda.amp.GradScaler()  # 使用混合精度训练\n",
    "    patience = 3  # 早停的耐心参数\n",
    "    early_stopping_counter = 0  # 早停计数器\n",
    "    \n",
    "    # 用于Kaggle输出的结果记录\n",
    "    results_history = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        \n",
    "        for batch_idx, batch in enumerate(progress_bar):\n",
    "            # 检查批次是否为空\n",
    "            if not batch or len(batch) == 0:\n",
    "                print(\"跳过空批次\")\n",
    "                continue\n",
    "                \n",
    "            # 检查输入IDs是否存在\n",
    "            if 'input_ids' not in batch:\n",
    "                print(f\"批次 {batch_idx} 中没有 input_ids，跳过\")\n",
    "                continue\n",
    "        \n",
    "        for batch_idx, batch in enumerate(progress_bar):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch.get('attention_mask', None)\n",
    "            if attention_mask is not None:\n",
    "                attention_mask = attention_mask.to(device)\n",
    "            src_padding_mask = batch.get('src_padding_mask', None)\n",
    "            if src_padding_mask is not None:\n",
    "                src_padding_mask = src_padding_mask.to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            task_types = batch['task_type']\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # 仅处理有效样本\n",
    "            valid_indices = (labels != -100).nonzero(as_tuple=True)[0]\n",
    "            \n",
    "            if len(valid_indices) > 0:\n",
    "                batch_input_ids = input_ids[valid_indices]\n",
    "                batch_attention_mask = attention_mask[valid_indices] if attention_mask is not None else None\n",
    "                batch_src_padding_mask = src_padding_mask[valid_indices] if src_padding_mask is not None else None\n",
    "                batch_labels = labels[valid_indices]\n",
    "                batch_task_types = [task_types[i] for i in valid_indices]\n",
    "                \n",
    "                # 使用混合精度训练\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    # 获取token_type_ids并移至设备\n",
    "                    token_type_ids = batch.get('token_type_ids', None)\n",
    "                    if token_type_ids is not None:\n",
    "                        # 先将索引移到与token_type_ids相同的设备上 (CPU)\n",
    "                        cpu_valid_indices = valid_indices.cpu()\n",
    "                        # 使用CPU上的索引切片CPU上的张量\n",
    "                        token_type_ids = token_type_ids[cpu_valid_indices]\n",
    "                        # 然后将结果移到目标设备\n",
    "                        token_type_ids = token_type_ids.to(device)\n",
    "                        \n",
    "                    outputs = model(\n",
    "                        input_ids=batch_input_ids,\n",
    "                        attention_mask=batch_attention_mask,\n",
    "                        token_type_ids=token_type_ids,\n",
    "                        task_type=batch_task_types\n",
    "                    )\n",
    "                    \n",
    "                    # 分开处理不同任务类型\n",
    "                    task_weights = {\n",
    "                        'classify': 1.0,\n",
    "                        'nli': 2.0,      # 增加NLI任务权重\n",
    "                        'mrc': 3.0       # 进一步增加MRC任务权重\n",
    "                    }\n",
    "                    \n",
    "                    # 初始化每种任务的损失\n",
    "                    losses = {}\n",
    "                    task_counts = {}\n",
    "                    \n",
    "                    # 按任务类型分组计算损失\n",
    "                    for task_type in set(batch_task_types):\n",
    "                        task_indices = [i for i, t in enumerate(batch_task_types) if t == task_type]\n",
    "                        task_counts[task_type] = len(task_indices)\n",
    "                        \n",
    "                        if task_type in ['classify', 'nli'] and task_indices:\n",
    "                            task_outputs = outputs[task_indices]\n",
    "                            task_labels = batch_labels[task_indices]\n",
    "                            \n",
    "                            # 使用标签平滑交叉熵来提高泛化能力\n",
    "                            if task_type == 'nli':\n",
    "                                # NLI任务可能需要更多正则化\n",
    "                                loss_fn = LabelSmoothingCrossEntropy(smoothing=0.1)\n",
    "                            else:\n",
    "                                loss_fn = torch.nn.CrossEntropyLoss(reduction='mean')\n",
    "                                \n",
    "                            losses[task_type] = loss_fn(task_outputs, task_labels) * task_weights[task_type]\n",
    "\n",
    "                    # 在train函数中找到损失计算部分，添加以下代码\n",
    "                    # 处理MRC任务\n",
    "                    mrc_indices = [i for i, t in enumerate(batch_task_types) if t == 'mrc']\n",
    "                    if mrc_indices:\n",
    "                        # 确保batch中包含start_position和end_position\n",
    "                        if 'start_position' in batch and 'end_position' in batch:\n",
    "                            # 获取MRC任务的开始和结束位置\n",
    "                            start_positions = batch['start_position'].to(device)\n",
    "                            end_positions = batch['end_position'].to(device)\n",
    "                            \n",
    "                            # 只选择MRC类型的样本\n",
    "                            mrc_start_positions = start_positions[mrc_indices]\n",
    "                            mrc_end_positions = end_positions[mrc_indices]\n",
    "                            \n",
    "                            # 获取MRC任务输入\n",
    "                            mrc_input_ids = batch_input_ids[mrc_indices]\n",
    "                            mrc_attention_mask = batch_attention_mask[mrc_indices] if batch_attention_mask is not None else None\n",
    "                            \n",
    "                            # 调用模型获取开始和结束位置的logits\n",
    "                            mrc_start_logits, mrc_end_logits = model(\n",
    "                                input_ids=mrc_input_ids,\n",
    "                                attention_mask=mrc_attention_mask,\n",
    "                                token_type_ids=token_type_ids[mrc_indices] if token_type_ids is not None else None,\n",
    "                                task_type='mrc'\n",
    "                            )\n",
    "                            \n",
    "                            # 计算MRC损失\n",
    "                            mrc_start_loss = F.cross_entropy(mrc_start_logits, mrc_start_positions)\n",
    "                            mrc_end_loss = F.cross_entropy(mrc_end_logits, mrc_end_positions)\n",
    "                            mrc_loss = (mrc_start_loss + mrc_end_loss) / 2\n",
    "                            \n",
    "                            # 添加到损失字典\n",
    "                            losses['mrc'] = mrc_loss * task_weights['mrc']\n",
    "\n",
    "                    # 计算总损失\n",
    "                    if losses:\n",
    "                        # 根据任务数量计算加权平均损失\n",
    "                        loss = sum(losses.values()) / len(losses)\n",
    "                    else:\n",
    "                        # 如果没有可计算的损失，使用一个小的虚拟损失\n",
    "                        loss = torch.tensor(0.01, device=device, requires_grad=True)\n",
    "                \n",
    "                # 使用混合精度训练优化反向传播\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                \n",
    "                scheduler.step()\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                global_step += 1\n",
    "            \n",
    "            progress_bar.set_postfix({\"Loss\": f\"{total_loss/(batch_idx+1):.4f}\"})\n",
    "            \n",
    "            if global_step % eval_steps == 0:                \n",
    "                val_results = evaluate(model, val_loader, device, tokenizer)\n",
    "                results_history.append({\n",
    "                    'step': global_step,\n",
    "                    'epoch': epoch+1,\n",
    "                    'val_results': val_results\n",
    "                })\n",
    "                \n",
    "                # 在Kaggle中显示美观的验证结果\n",
    "                display(HTML(f\"\"\"\n",
    "                <h4>Validation Results at Step {global_step}</h4>\n",
    "                <ul>\n",
    "                    <li>Overall Accuracy: {val_results.get('overall_accuracy', 0):.4f}</li>\n",
    "                    <li>Classify Accuracy: {val_results.get('classify_accuracy', 0):.4f}</li>\n",
    "                    <li>NLI Accuracy: {val_results.get('nli_accuracy', 0):.4f}</li>\n",
    "                    <li>MRC Exact Match: {val_results.get('mrc_em', 0):.4f}</li>\n",
    "                </ul>\n",
    "                \"\"\"))\n",
    "                \n",
    "                model.train()\n",
    "                \n",
    "                val_score = sum(val_results.values()) / len(val_results) if val_results else 0\n",
    "                if val_score > best_val_score:\n",
    "                    best_val_score = val_score\n",
    "                    torch.save(model.state_dict(), best_model_path)\n",
    "                    print(f\"New best model saved with score: {best_val_score:.4f} at {best_model_path}\")\n",
    "        \n",
    "        # 每个epoch结束后的评估\n",
    "        val_results = evaluate(model, val_loader, device, tokenizer)\n",
    "        results_history.append({\n",
    "            'epoch': epoch+1,\n",
    "            'val_results': val_results\n",
    "        })\n",
    "        \n",
    "        display(HTML(f\"\"\"\n",
    "        <h3>End of Epoch {epoch+1} Validation Results</h3>\n",
    "        <ul>\n",
    "            <li>Overall Accuracy: {val_results.get('overall_accuracy', 0):.4f}</li>\n",
    "            <li>Classify Accuracy: {val_results.get('classify_accuracy', 0):.4f}</li>\n",
    "            <li>NLI Accuracy: {val_results.get('nli_accuracy', 0):.4f}</li>\n",
    "            <li>MRC Exact Match: {val_results.get('mrc_em', 0):.4f}</li>\n",
    "        </ul>\n",
    "        \"\"\"))\n",
    "        \n",
    "        val_score = sum(val_results.values()) / len(val_results) if val_results else 0\n",
    "        if val_score > best_val_score:\n",
    "            best_val_score = val_score\n",
    "            torch.save(model.state_dict(), best_model_path)\n",
    "            print(f\"New best model saved with score: {best_val_score:.4f} at {best_model_path}\")\n",
    "            \n",
    "        # GPU内存回收\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    return results_history\n",
    "\n",
    "def train_multitask(model, train_loaders, val_loaders, optimizer, scheduler, device, \n",
    "                   best_model_path, tokenizer, num_epochs=3, eval_steps=100):\n",
    "    \"\"\"按任务类型分别训练模型\"\"\"\n",
    "    best_val_score = 0\n",
    "    global_step = 0\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "    patience = 3\n",
    "    early_stopping_counter = 0\n",
    "    \n",
    "    # 记录结果历史\n",
    "    results_history = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        batch_count = 0\n",
    "        \n",
    "        # 为每个任务创建迭代器\n",
    "        task_iterators = {task: iter(loader) for task, loader in train_loaders.items()}\n",
    "        active_tasks = list(task_iterators.keys())\n",
    "        \n",
    "        # 估计总批次数\n",
    "        total_batches = sum(len(loader) for loader in train_loaders.values())\n",
    "        progress_bar = tqdm(range(total_batches), desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        \n",
    "        # 任务权重\n",
    "        task_weights = {\n",
    "            'classify': 1.0,\n",
    "            'nli': 1.5,\n",
    "            'mrc': 5.0\n",
    "        }\n",
    "        \n",
    "        # 按序轮流训练各任务\n",
    "        while active_tasks:\n",
    "            for task_type in list(active_tasks):  # 使用list创建副本以便修改\n",
    "                try:\n",
    "                    batch = next(task_iterators[task_type])\n",
    "                    \n",
    "                    # 处理批次\n",
    "                    input_ids = batch['input_ids'].to(device)\n",
    "                    attention_mask = batch.get('attention_mask', None)\n",
    "                    if attention_mask is not None:\n",
    "                        attention_mask = attention_mask.to(device)\n",
    "                    \n",
    "                    token_type_ids = batch.get('token_type_ids', None)\n",
    "                    if token_type_ids is not None:\n",
    "                        token_type_ids = token_type_ids.to(device)\n",
    "                    \n",
    "                    # 针对不同任务类型的处理\n",
    "                    optimizer.zero_grad()\n",
    "                    \n",
    "                    with torch.cuda.amp.autocast():\n",
    "                        if task_type in ['classify', 'nli']:\n",
    "                            labels = batch['label'].to(device)\n",
    "                            outputs = model(\n",
    "                                input_ids=input_ids,\n",
    "                                attention_mask=attention_mask,\n",
    "                                token_type_ids=token_type_ids,\n",
    "                                task_type=task_type\n",
    "                            )\n",
    "                            \n",
    "                            # 选择损失函数\n",
    "                            if task_type == 'nli':\n",
    "                                loss_fn = LabelSmoothingCrossEntropy(smoothing=0.1)\n",
    "                            else:\n",
    "                                loss_fn = torch.nn.CrossEntropyLoss(reduction='mean')\n",
    "                                \n",
    "                            loss = loss_fn(outputs, labels) * task_weights[task_type]\n",
    "                            \n",
    "                        elif task_type == 'mrc':\n",
    "                            start_positions = batch['start_position'].to(device)\n",
    "                            end_positions = batch['end_position'].to(device)\n",
    "                            \n",
    "                            start_logits, end_logits = model(\n",
    "                                input_ids=input_ids,\n",
    "                                attention_mask=attention_mask,\n",
    "                                token_type_ids=token_type_ids,\n",
    "                                task_type='mrc'\n",
    "                            )\n",
    "                            \n",
    "                            # 计算MRC损失\n",
    "                            start_loss = F.cross_entropy(start_logits, start_positions)\n",
    "                            end_loss = F.cross_entropy(end_logits, end_positions)\n",
    "                            loss = (start_loss + end_loss) / 2 * task_weights[task_type]\n",
    "                    \n",
    "                    # 优化步骤\n",
    "                    scaler.scale(loss).backward()\n",
    "                    scaler.unscale_(optimizer)\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                    scaler.step(optimizer)\n",
    "                    scaler.update()\n",
    "                    \n",
    "                    scheduler.step()\n",
    "                    \n",
    "                    total_loss += loss.item()\n",
    "                    batch_count += 1\n",
    "                    global_step += 1\n",
    "                    \n",
    "                    # 更新进度条\n",
    "                    progress_bar.update(1)\n",
    "                    progress_bar.set_postfix({\"Loss\": f\"{total_loss/batch_count:.4f}\"})\n",
    "                    \n",
    "                    # 周期性评估\n",
    "                    if global_step % eval_steps == 0:\n",
    "                        val_results = evaluate_multitask(model, val_loaders, device, tokenizer)\n",
    "                        results_history.append({\n",
    "                            'step': global_step,\n",
    "                            'epoch': epoch+1,\n",
    "                            'val_results': val_results\n",
    "                        })\n",
    "                        \n",
    "                        # 显示结果\n",
    "                        display(HTML(f\"\"\"\n",
    "                        <h4>Validation Results at Step {global_step}</h4>\n",
    "                        <ul>\n",
    "                            <li>Overall Accuracy: {val_results.get('overall_accuracy', 0):.4f}</li>\n",
    "                            <li>Classify Accuracy: {val_results.get('classify_accuracy', 0):.4f}</li>\n",
    "                            <li>NLI Accuracy: {val_results.get('nli_accuracy', 0):.4f}</li>\n",
    "                            <li>MRC Exact Match: {val_results.get('mrc_em', 0):.4f}</li>\n",
    "                        </ul>\n",
    "                        \"\"\"))\n",
    "                        \n",
    "                        model.train()\n",
    "                        \n",
    "                        # 保存最佳模型\n",
    "                        val_score = sum(val_results.values()) / len(val_results) if val_results else 0\n",
    "                        if val_score > best_val_score:\n",
    "                            best_val_score = val_score\n",
    "                            torch.save(model.state_dict(), best_model_path)\n",
    "                            print(f\"New best model saved with score: {best_val_score:.4f}\")\n",
    "                            early_stopping_counter = 0\n",
    "                        else:\n",
    "                            early_stopping_counter += 1\n",
    "                            \n",
    "                except StopIteration:\n",
    "                    # 当前任务的批次已用完\n",
    "                    active_tasks.remove(task_type)\n",
    "        \n",
    "        # 每个epoch结束后的评估\n",
    "        val_results = evaluate_multitask(model, val_loaders, device, tokenizer)\n",
    "        results_history.append({\n",
    "            'epoch': epoch+1,\n",
    "            'val_results': val_results\n",
    "        })\n",
    "        \n",
    "        display(HTML(f\"\"\"\n",
    "        <h3>End of Epoch {epoch+1} Validation Results</h3>\n",
    "        <ul>\n",
    "            <li>Overall Accuracy: {val_results.get('overall_accuracy', 0):.4f}</li>\n",
    "            <li>Classify Accuracy: {val_results.get('classify_accuracy', 0):.4f}</li>\n",
    "            <li>NLI Accuracy: {val_results.get('nli_accuracy', 0):.4f}</li>\n",
    "            <li>MRC Exact Match: {val_results.get('mrc_em', 0):.4f}</li>\n",
    "        </ul>\n",
    "        \"\"\"))\n",
    "        \n",
    "        val_score = sum(val_results.values()) / len(val_results) if val_results else 0\n",
    "        if val_score > best_val_score:\n",
    "            best_val_score = val_score\n",
    "            torch.save(model.state_dict(), best_model_path)\n",
    "            print(f\"New best model saved with score: {best_val_score:.4f}\")\n",
    "        \n",
    "        # 早停检查\n",
    "        if early_stopping_counter >= patience:\n",
    "            print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
    "            break\n",
    "            \n",
    "        # GPU内存回收\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    return results_history\n",
    "\n",
    "# 评估函数 - 修改以支持MRC任务\n",
    "def evaluate(model, dataloader, device, tokenizer):\n",
    "    model.eval()\n",
    "    \n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_task_types = []\n",
    "    mrc_results = []  # 存储MRC任务的结果\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch.get('attention_mask', None)\n",
    "            if attention_mask is not None:\n",
    "                attention_mask = attention_mask.to(device)\n",
    "            src_padding_mask = batch.get('src_padding_mask', None)\n",
    "            if src_padding_mask is not None:\n",
    "                src_padding_mask = src_padding_mask.to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            task_types = batch['task_type']\n",
    "            target_texts = batch['target_text']\n",
    "            \n",
    "            valid_indices = (labels != -100).nonzero(as_tuple=True)[0]\n",
    "            mrc_indices = [i for i, t in enumerate(task_types) if t == 'mrc']\n",
    "            \n",
    "            # 处理分类和NLI任务\n",
    "            if len(valid_indices) > 0:\n",
    "                batch_input_ids = input_ids[valid_indices]\n",
    "                batch_attention_mask = attention_mask[valid_indices] if attention_mask is not None else None\n",
    "                batch_src_padding_mask = src_padding_mask[valid_indices] if src_padding_mask is not None else None\n",
    "                batch_labels = labels[valid_indices]\n",
    "                batch_task_types = [task_types[i] for i in valid_indices]\n",
    "                \n",
    "                # 只处理非MRC任务\n",
    "                non_mrc_indices = [i for i, t in enumerate(batch_task_types) if t != 'mrc']\n",
    "                if non_mrc_indices:\n",
    "                    non_mrc_input_ids = batch_input_ids[non_mrc_indices]\n",
    "                    non_mrc_attention_mask = batch_attention_mask[non_mrc_indices] if batch_attention_mask is not None else None\n",
    "                    non_mrc_token_type_ids = batch.get('token_type_ids', None)\n",
    "                    if non_mrc_token_type_ids is not None:\n",
    "                        cpu_valid_indices = valid_indices.cpu()\n",
    "                        non_mrc_token_type_ids = non_mrc_token_type_ids[cpu_valid_indices][non_mrc_indices].to(device)\n",
    "                    \n",
    "                    non_mrc_task_types = [batch_task_types[i] for i in non_mrc_indices]\n",
    "                    non_mrc_labels = batch_labels[non_mrc_indices]\n",
    "                    \n",
    "                    with torch.cuda.amp.autocast():\n",
    "                        outputs = model(\n",
    "                            input_ids=non_mrc_input_ids,\n",
    "                            attention_mask=non_mrc_attention_mask,\n",
    "                            token_type_ids=non_mrc_token_type_ids,\n",
    "                            task_type=non_mrc_task_types\n",
    "                        )\n",
    "                    \n",
    "                    preds = torch.argmax(outputs, dim=-1)\n",
    "                    preds = preds.cpu().numpy()\n",
    "                    non_mrc_labels = non_mrc_labels.cpu().numpy()\n",
    "                    \n",
    "                    all_preds.extend(preds)\n",
    "                    all_labels.extend(non_mrc_labels)\n",
    "                    all_task_types.extend(non_mrc_task_types)\n",
    "            \n",
    "            # 单独处理MRC任务\n",
    "            if mrc_indices:\n",
    "                for i in mrc_indices:\n",
    "                    mrc_input_ids = input_ids[i:i+1]\n",
    "                    mrc_attention_mask = attention_mask[i:i+1] if attention_mask is not None else None\n",
    "                    mrc_token_type_ids = batch.get('token_type_ids', None)\n",
    "                    if mrc_token_type_ids is not None:\n",
    "                        mrc_token_type_ids = mrc_token_type_ids[i:i+1].to(device)\n",
    "                    \n",
    "                    with torch.cuda.amp.autocast():\n",
    "                        start_logits, end_logits = model(\n",
    "                            input_ids=mrc_input_ids,\n",
    "                            attention_mask=mrc_attention_mask,\n",
    "                            token_type_ids=mrc_token_type_ids,\n",
    "                            task_type='mrc'\n",
    "                        )\n",
    "                    \n",
    "                    # 获取最可能的起始和结束位置\n",
    "                    start_idx = torch.argmax(start_logits, dim=-1).item()\n",
    "                    end_idx = torch.argmax(end_logits, dim=-1).item()\n",
    "                    \n",
    "                    # 确保end_idx >= start_idx\n",
    "                    if end_idx < start_idx:\n",
    "                        end_idx = start_idx\n",
    "                    \n",
    "                    # 将token ids转换为文本\n",
    "                    predicted_answer = tokenizer.decode(mrc_input_ids[0][start_idx:end_idx+1].tolist(),\n",
    "                                                     skip_special_tokens=True)\n",
    "                    \n",
    "                    # 计算精确匹配分数 (EM)\n",
    "                    target_text = target_texts[i]\n",
    "                    is_exact_match = predicted_answer.strip() == target_text.strip()\n",
    "                    \n",
    "                    mrc_results.append({\n",
    "                        'predicted': predicted_answer,\n",
    "                        'target': target_text,\n",
    "                        'exact_match': is_exact_match\n",
    "                    })\n",
    "    \n",
    "    results = {}\n",
    "    if all_preds:\n",
    "        overall_acc = accuracy_score(all_labels, all_preds)\n",
    "        results['overall_accuracy'] = overall_acc\n",
    "        \n",
    "        # 计算F1分数\n",
    "        f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "        results['overall_f1'] = f1\n",
    "    \n",
    "    # 计算任务特定指标\n",
    "    task_metrics = {}\n",
    "    for task_type in set(all_task_types):\n",
    "        task_indices = [i for i, t in enumerate(all_task_types) if t == task_type]\n",
    "        if task_indices:\n",
    "            task_preds = [all_preds[i] for i in task_indices]\n",
    "            task_labels = [all_labels[i] for i in task_indices]\n",
    "            task_acc = accuracy_score(task_labels, task_preds)\n",
    "            task_f1 = f1_score(task_labels, task_preds, average='weighted')\n",
    "            \n",
    "            results[f'{task_type}_accuracy'] = task_acc\n",
    "            results[f'{task_type}_f1'] = task_f1\n",
    "            \n",
    "            task_metrics[task_type] = {\n",
    "                'accuracy': task_acc,\n",
    "                'f1': task_f1,\n",
    "                'support': len(task_indices)\n",
    "            }\n",
    "    \n",
    "    # 计算MRC任务的精确匹配分数\n",
    "    if mrc_results:\n",
    "        mrc_em = sum(item['exact_match'] for item in mrc_results) / len(mrc_results)\n",
    "        results['mrc_em'] = mrc_em\n",
    "        task_metrics['mrc'] = {\n",
    "            'exact_match': mrc_em,\n",
    "            'support': len(mrc_results)\n",
    "        }\n",
    "    \n",
    "    # 在Kaggle中显示详细结果表格\n",
    "    if task_metrics:\n",
    "        metrics_df = pd.DataFrame.from_dict(task_metrics, orient='index')\n",
    "        display(HTML(\"<h4>Detailed Task Metrics</h4>\"))\n",
    "        display(metrics_df)\n",
    "    \n",
    "    return results\n",
    "\n",
    "def calculate_mrc_metrics(predictions, targets):\n",
    "    \"\"\"计算MRC的多种评估指标\"\"\"\n",
    "    exact_matches = 0\n",
    "    f1_scores = 0\n",
    "    partial_matches = 0  # 部分重叠\n",
    "    \n",
    "    for pred, target in zip(predictions, targets):\n",
    "        # 精确匹配\n",
    "        if pred.strip() == target.strip():\n",
    "            exact_matches += 1\n",
    "            f1_scores += 1\n",
    "            partial_matches += 1\n",
    "            continue\n",
    "            \n",
    "        # 计算F1和部分匹配\n",
    "        if len(pred) > 0:\n",
    "            # 字符级别F1\n",
    "            common_chars = set(pred) & set(target)\n",
    "            if common_chars:\n",
    "                precision = len(common_chars) / len(pred)\n",
    "                recall = len(common_chars) / len(target)\n",
    "                f1 = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "                f1_scores += f1\n",
    "                \n",
    "                # 部分重叠：如果有一定比例的字符重叠\n",
    "                if f1 >= 0.5:\n",
    "                    partial_matches += 1\n",
    "            \n",
    "    num_samples = len(predictions)\n",
    "    if num_samples == 0:\n",
    "        return 0, 0, 0\n",
    "    \n",
    "    return (\n",
    "        exact_matches / num_samples,  # 精确匹配率\n",
    "        f1_scores / num_samples,      # 平均F1\n",
    "        partial_matches / num_samples # 部分匹配率\n",
    "    )\n",
    "\n",
    "def evaluate_multitask(model, val_loaders, device, tokenizer):\n",
    "    \"\"\"评估多任务模型\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    task_results = {}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for task_type, loader in val_loaders.items():\n",
    "            if task_type in ['classify', 'nli']:\n",
    "                all_preds = []\n",
    "                all_labels = []\n",
    "                \n",
    "                for batch in tqdm(loader, desc=f\"Evaluating {task_type}\"):\n",
    "                    input_ids = batch['input_ids'].to(device)\n",
    "                    attention_mask = batch.get('attention_mask', None)\n",
    "                    if attention_mask is not None:\n",
    "                        attention_mask = attention_mask.to(device)\n",
    "                    token_type_ids = batch.get('token_type_ids', None)\n",
    "                    if token_type_ids is not None:\n",
    "                        token_type_ids = token_type_ids.to(device)\n",
    "                    labels = batch['label'].to(device)\n",
    "                    \n",
    "                    outputs = model(\n",
    "                        input_ids=input_ids,\n",
    "                        attention_mask=attention_mask,\n",
    "                        token_type_ids=token_type_ids,\n",
    "                        task_type=task_type\n",
    "                    )\n",
    "                    \n",
    "                    preds = torch.argmax(outputs, dim=-1)\n",
    "                    all_preds.extend(preds.cpu().numpy())\n",
    "                    all_labels.extend(labels.cpu().numpy())\n",
    "                \n",
    "                if all_preds:\n",
    "                    accuracy = accuracy_score(all_labels, all_preds)\n",
    "                    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "                    task_results[f'{task_type}_accuracy'] = accuracy\n",
    "                    task_results[f'{task_type}_f1'] = f1\n",
    "                    \n",
    "            elif task_type == 'mrc':\n",
    "                mrc_results = []\n",
    "                \n",
    "                for batch in tqdm(loader, desc=\"Evaluating MRC\"):\n",
    "                    input_ids = batch['input_ids'].to(device)\n",
    "                    attention_mask = batch.get('attention_mask', None)\n",
    "                    if attention_mask is not None:\n",
    "                        attention_mask = attention_mask.to(device)\n",
    "                    token_type_ids = batch.get('token_type_ids', None)\n",
    "                    if token_type_ids is not None:\n",
    "                        token_type_ids = token_type_ids.to(device)\n",
    "                    target_texts = batch['target_text']\n",
    "                    \n",
    "                    start_logits, end_logits = model(\n",
    "                        input_ids=input_ids,\n",
    "                        attention_mask=attention_mask,\n",
    "                        token_type_ids=token_type_ids,\n",
    "                        task_type='mrc'\n",
    "                    )\n",
    "                    \n",
    "                    # 对每个样本计算预测答案\n",
    "                    for i in range(input_ids.size(0)):\n",
    "                        sample_start_logits = start_logits[i]\n",
    "                        sample_end_logits = end_logits[i]\n",
    "                        \n",
    "                        # 获取最可能的起始和结束位置\n",
    "                        start_idx = torch.argmax(sample_start_logits).item()\n",
    "                        end_idx = torch.argmax(sample_end_logits).item()\n",
    "                        \n",
    "                        # 确保end_idx >= start_idx\n",
    "                        if end_idx < start_idx:\n",
    "                            end_idx = start_idx\n",
    "                            \n",
    "                        # 限制答案长度\n",
    "                        if end_idx - start_idx > 30:\n",
    "                            end_idx = start_idx + 30\n",
    "                        \n",
    "                        # 将token ids转换为文本\n",
    "                        predicted_answer = tokenizer.decode(\n",
    "                            input_ids[i][start_idx:end_idx+1].tolist(),\n",
    "                            skip_special_tokens=True\n",
    "                        )\n",
    "                        \n",
    "                        # 计算精确匹配\n",
    "                        target_text = target_texts[i]\n",
    "                        is_exact_match = predicted_answer.strip() == target_text.strip()\n",
    "                        \n",
    "                        mrc_results.append({\n",
    "                            'predicted': predicted_answer,\n",
    "                            'target': target_text,\n",
    "                            'exact_match': is_exact_match\n",
    "                        })\n",
    "                \n",
    "                # 记录更详细的结果\n",
    "                if mrc_results:\n",
    "                    predicted_texts = [item['predicted'] for item in mrc_results]\n",
    "                    target_texts = [item['target'] for item in mrc_results]\n",
    "                    \n",
    "                    # 计算多种指标\n",
    "                    exact_match, f1, partial_match = calculate_mrc_metrics(predicted_texts, target_texts)\n",
    "                    \n",
    "                    # 展示详细诊断信息\n",
    "                    print(f\"\\n【MRC诊断信息】\")\n",
    "                    print(f\"样本数: {len(mrc_results)}\")\n",
    "                    print(f\"精确匹配率: {exact_match:.4f}\")\n",
    "                    print(f\"平均F1分数: {f1:.4f}\")\n",
    "                    print(f\"部分匹配率: {partial_match:.4f}\")\n",
    "                    \n",
    "                    # 展示一些样例\n",
    "                    print(\"\\n【预测样例】\")\n",
    "                    for i in range(min(5, len(mrc_results))):\n",
    "                        print(f\"目标: '{mrc_results[i]['target']}'\")\n",
    "                        print(f\"预测: '{mrc_results[i]['predicted']}'\")\n",
    "                        print(f\"匹配: {mrc_results[i]['exact_match']}\\n\")\n",
    "                    \n",
    "                    task_results['mrc_em'] = exact_match\n",
    "                    task_results['mrc_f1'] = f1\n",
    "                    task_results['mrc_partial'] = partial_match\n",
    "    \n",
    "    # 计算总体结果\n",
    "    if task_results:\n",
    "        task_results['overall_accuracy'] = sum(\n",
    "            value for key, value in task_results.items() \n",
    "            if key.endswith('_accuracy') or key.endswith('_em')\n",
    "        ) / sum(1 for key in task_results if key.endswith('_accuracy') or key.endswith('_em'))\n",
    "    \n",
    "    # 显示详细结果表格\n",
    "    metrics_df = pd.DataFrame(\n",
    "        [[task_results.get(f'{task}_accuracy', None), \n",
    "          task_results.get(f'{task}_f1', None), \n",
    "          task_results.get(f'{task}_em', None)] \n",
    "         for task in ['classify', 'nli', 'mrc']],\n",
    "        index=['Classify', 'NLI', 'MRC'],\n",
    "        columns=['Accuracy/EM', 'F1', 'EM']\n",
    "    )\n",
    "    display(HTML(\"<h4>Detailed Task Metrics</h4>\"))\n",
    "    display(metrics_df)\n",
    "    \n",
    "    return task_results\n",
    "\n",
    "def main():\n",
    "    # 参数设置\n",
    "    # 检测运行环境并设置相应的路径\n",
    "    is_kaggle = os.path.exists('/kaggle') and '/kaggle/working' in os.getcwd()\n",
    "\n",
    "    # 确保使用正确的项目根目录\n",
    "    project_root = os.path.dirname(os.path.abspath('__file__'))\n",
    "    possible_paths = [\n",
    "        os.path.dirname(os.path.abspath('__file__')),  # 当前文件所在目录\n",
    "        os.getcwd(),  # 当前工作目录\n",
    "        '/root/Code/Multi-task Project'  # 从错误消息推断的完整路径\n",
    "    ]\n",
    "\n",
    "    for path in possible_paths:\n",
    "        if os.path.exists(path):\n",
    "            project_root = path\n",
    "            break\n",
    "\n",
    "    print(f\"Using project root: {project_root}\")\n",
    "\n",
    "    # 定义best_model_path变量\n",
    "    if is_kaggle:\n",
    "        model_path = '/kaggle/working/best_multitask_transformer.pt'\n",
    "        best_model_path = model_path\n",
    "        output_dir = '/kaggle/working/outputs'\n",
    "        data_file = '/kaggle/input/multitask-data/data.jsonl'\n",
    "    else:\n",
    "        model_paths = [\n",
    "            os.path.join(project_root, \"outputs\", \"transformer_model\", \"best_multitask_transformer.pt\"),\n",
    "            os.path.join(project_root, \"test_outputs\", \"multitask_model.pth\"),\n",
    "            os.path.join(project_root, \"best_multitask_transformer.pt\"),\n",
    "            \"./best_multitask_transformer.pt\"\n",
    "        ]\n",
    "        \n",
    "        model_path = None\n",
    "        for path in model_paths:\n",
    "            if os.path.exists(path):\n",
    "                model_path = path\n",
    "                break\n",
    "        \n",
    "        if model_path is None:\n",
    "            model_path = os.path.join(project_root, \"outputs\", \"transformer_model\", \"best_multitask_transformer.pt\")\n",
    "            print(f\"警告: 没有找到现有模型，将创建一个新模型实例\")\n",
    "        \n",
    "        best_model_path = model_path\n",
    "        output_dir = os.path.join(project_root, \"outputs\", \"transformer_model\")\n",
    "        data_file = os.path.join(project_root, \"data/processed\", \"pCLUE_train.json\") \n",
    "\n",
    "    print(f\"使用模型路径: {model_path}\")\n",
    "    \n",
    "    # 模型训练参数\n",
    "    sample_percentage = 80\n",
    "    max_samples = 45000\n",
    "    batch_size = 16  # 增加批次大小，因为现在每种任务独立处理\n",
    "    epochs = 8\n",
    "    lr = 3e-5\n",
    "    weight_decay = 0.01\n",
    "    eval_steps = 200\n",
    "    seed = 42\n",
    "    task_types = [\"classify\", \"nli\", \"mrc\"]\n",
    "    \n",
    "    # 确保输出目录存在\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # 检查数据文件是否存在\n",
    "    if not os.path.exists(data_file):\n",
    "        raise FileNotFoundError(f\"数据文件 '{data_file}' 不存在，请检查路径\")\n",
    "    \n",
    "    set_seed(seed)\n",
    "    device = get_device()\n",
    "    \n",
    "    # 显示开始信息\n",
    "    display(HTML(\"<h2>Starting Multitask Training with Task-Specific Data Loading</h2>\"))\n",
    "    \n",
    "    # 加载数据\n",
    "    all_data = load_json_data_with_sampling(\n",
    "        data_file,\n",
    "        sample_percentage=sample_percentage,\n",
    "        max_samples=max_samples,\n",
    "        task_types=task_types\n",
    "    )\n",
    "    \n",
    "    # 确保每个样本都有答案选项\n",
    "    for item in all_data:\n",
    "        if 'answer_choices' not in item or not item['answer_choices']:\n",
    "            item['answer_choices'] = [\"是的\", \"不是\"] if item['type'] == 'nli' else [\"选项A\", \"选项B\"]\n",
    "    \n",
    "    # 分割数据集\n",
    "    train_size = int(0.8 * len(all_data))\n",
    "    random.shuffle(all_data)\n",
    "    train_data = all_data[:train_size]\n",
    "    val_data = all_data[train_size:]\n",
    "    \n",
    "    # 加载tokenizer\n",
    "    local_model_path = \"./bert-base-chinese-local\"\n",
    "    if os.path.exists(os.path.join(local_model_path, \"tokenizer_config.json\")):\n",
    "        tokenizer = AutoTokenizer.from_pretrained(local_model_path, local_files_only=True)\n",
    "    else:\n",
    "        alternative_paths = [\n",
    "            os.path.join(project_root, \"bert-base-chinese-local\"),\n",
    "            \"/root/Code/Multi-task Project/bert-base-chinese-local\"\n",
    "        ]\n",
    "        found_model = False\n",
    "        for path in alternative_paths:\n",
    "            if os.path.exists(os.path.join(path, \"tokenizer_config.json\")):\n",
    "                tokenizer = AutoTokenizer.from_pretrained(path, local_files_only=True)\n",
    "                found_model = True\n",
    "                break\n",
    "        \n",
    "        if not found_model:\n",
    "            raise ValueError(\"无法找到本地模型文件，请确认bert-base-chinese-local目录路径\")\n",
    "    \n",
    "    # 创建任务特定的数据加载器\n",
    "    train_loaders, val_loaders, label_maps = create_task_specific_dataloaders(\n",
    "        train_data, val_data, tokenizer, batch_size\n",
    "    )\n",
    "    \n",
    "    # 计算词汇表大小和标签数量\n",
    "    vocab_size = tokenizer.vocab_size\n",
    "    num_labels = max(len(label_map) for label_map in label_maps.values()) if label_maps else 2\n",
    "    print(f\"Vocabulary size: {vocab_size}\")\n",
    "    print(f\"Max labels for classification: {num_labels}\")\n",
    "    \n",
    "    # 创建模型\n",
    "    model = MultitaskBertModel(num_labels=num_labels)\n",
    "    model.to(device)\n",
    "    \n",
    "    # 优化器和学习率调度器设置\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = ['bias', 'LayerNorm.weight']\n",
    "    \n",
    "    task_head_params = {\n",
    "        'classify': list(model.classify_head.parameters()),\n",
    "        'nli': list(model.nli_head.parameters()),\n",
    "        'mrc': list(model.mrc_qa_outputs.parameters()) + list(model.mrc_lstm.parameters())\n",
    "    }\n",
    "    \n",
    "    optimizer_grouped_parameters = [\n",
    "        # BERT层参数\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay) and 'bert' in n],\n",
    "        'weight_decay': weight_decay, 'lr': lr},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay) and 'bert' in n],\n",
    "        'weight_decay': 0.0, 'lr': lr},\n",
    "        # 分类任务头参数\n",
    "        {'params': task_head_params['classify'],\n",
    "        'weight_decay': weight_decay, 'lr': lr * 10},\n",
    "        # NLI任务头参数\n",
    "        {'params': task_head_params['nli'],\n",
    "        'weight_decay': weight_decay, 'lr': lr * 20},\n",
    "        # MRC任务头参数\n",
    "        {'params': task_head_params['mrc'],\n",
    "        'weight_decay': weight_decay, 'lr': lr * 50}\n",
    "    ]\n",
    "    \n",
    "    optimizer = AdamW(optimizer_grouped_parameters)\n",
    "    \n",
    "    # 计算总步数\n",
    "    total_steps = sum(len(loader) for loader in train_loaders.values()) * epochs\n",
    "    warmup_steps = total_steps // 10  # 10%的预热步骤\n",
    "    \n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, \n",
    "        num_warmup_steps=warmup_steps,\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "    \n",
    "    # 训练模型\n",
    "    results_history = train_multitask(\n",
    "        model,\n",
    "        train_loaders,\n",
    "        val_loaders,\n",
    "        optimizer,\n",
    "        scheduler,\n",
    "        device,\n",
    "        best_model_path,\n",
    "        tokenizer,\n",
    "        num_epochs=epochs,\n",
    "        eval_steps=eval_steps\n",
    "    )\n",
    "    \n",
    "    # 最终评估\n",
    "    if os.path.exists(best_model_path):\n",
    "        print(f\"加载最佳模型用于最终评估: {best_model_path}\")\n",
    "        model.load_state_dict(torch.load(best_model_path))\n",
    "        model.to(device)\n",
    "    else:\n",
    "        print(f\"警告：无法找到最佳模型文件，使用当前模型进行评估\")\n",
    "    \n",
    "    final_results = evaluate_multitask(model, val_loaders, device, tokenizer)\n",
    "    \n",
    "    display(HTML(\"<h2>Final Evaluation Results</h2>\"))\n",
    "    \n",
    "    # 保存最终配置和结果\n",
    "    # 提取每个任务的标签映射\n",
    "    label_maps_inv = {\n",
    "        task: {v: k for k, v in task_map.items()} \n",
    "        for task, task_map in label_maps.items()\n",
    "    }\n",
    "    with open(os.path.join(output_dir, \"label_maps.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(label_maps_inv, f, ensure_ascii=False, indent=2)\n",
    "    print(\"Task-specific label maps saved for inference\")\n",
    "    \n",
    "    # 保存模型配置信息\n",
    "    config_info = {\n",
    "        \"vocab_size\": vocab_size,\n",
    "        \"num_labels\": num_labels,\n",
    "        \"task_types\": task_types,\n",
    "        \"training_params\": {\n",
    "            \"batch_size\": batch_size,\n",
    "            \"epochs\": epochs,\n",
    "            \"learning_rate\": lr\n",
    "        },\n",
    "        \"best_results\": final_results\n",
    "    }\n",
    "    \n",
    "    with open(os.path.join(output_dir, \"model_config.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(config_info, f, ensure_ascii=False, indent=2)\n",
    "    print(\"Model configuration saved\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5dc32a",
   "metadata": {},
   "source": [
    "# 4"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
